# Cab Demand Prediction System for Bangalore

A machine learning-based system for predicting cab demand in Bangalore using geospatial and temporal data. This project helps optimize cab supply by identifying high-demand zones at specific times, enabling better resource allocation and reduced wait times.

## Table of Contents
- [Overview](#overview)
- [Methodology](#methodology)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Usage](#usage)
- [API Documentation](#api-documentation)
- [Models](#models)
- [Datasets](#datasets)
- [Features](#features)

## Overview

This system predicts cab demand across Bangalore by analyzing:
- **Geospatial data**: Latitude and longitude coordinates
- **Temporal patterns**: Time of day converted to minutes since midnight
- **Location types**: Metro stations, parking lots, apartments, and company locations

The project uses Random Forest Classification to predict whether a location will have **High** or **Low** demand at a given time.

## Methodology

### 1. Data Collection & Preprocessing

**Data Sources:**
- `bengaluru-metro-stations.csv` - Metro station locations
- `Cleaned_bangalore_parking.csv` - Parking lot locations with timestamps
- `modified_apartment_data.csv` - Residential apartment locations with timing data
- `company_data.csv` - Corporate office locations
- `Cleaned_RideReqTimeData.csv` - Historical ride request data

**Preprocessing Steps:**
1. **Time Conversion**: Convert time strings (HH:MM) to minutes since midnight
   ```python
   time_minutes = hour * 60 + minutes
   ```

2. **Feature Engineering**: Extract three key features:
   - Latitude (float)
   - Longitude (float)
   - Time_Minutes (integer: 0-1439)

3. **Label Generation**: Create binary classification labels
   - `1` = High Demand (peak hours)
   - `0` = Low Demand (off-peak hours)

### 2. Model Training Approaches

Four different models were trained using various data sources and strategies:

#### **Model 1** (`demand_prediction_model1.pkl`)
- **Data Source**: Apartment locations with timing data
- **Strategy**: Balanced dataset with positive and negative samples
  - Positive samples: Original apartment data with actual timings
  - Negative samples: Same locations with randomly shifted times to non-peak hours
- **Accuracy**: ~94.9%
- **Use Case**: Residential area demand prediction

#### **Model 2** (`demand_prediction_model2.pkl`)
- **Data Source**: Parking lot locations with timestamps
- **Strategy**: Synthetic demand zones
  - Peak hours defined: 8:00-11:00 AM
  - Non-demand samples generated by shifting times outside peak hours
- **Accuracy**: ~44.9% (lower due to more complex patterns)
- **Use Case**: Parking area and transit hub demand

#### **Model 3** (`demand_prediction.pkl`)
- **Data Source**: Company/corporate locations
- **Strategy**: Evening rush hour focus
  - High demand: 5:00-7:00 PM (1020-1140 minutes)
  - Low demand: All other times
- **Accuracy**: ~95%
- **Use Case**: Corporate area evening demand prediction

#### **Model 4** (`demand_prediction_model.pkl`)
- **Data Source**: Metro station locations
- **Strategy**: Dual peak hours
  - Morning peak: 8:45-10:20 AM
  - Evening peak: 4:20-7:00 PM
  - Synthetic data generated at 10-minute intervals
- **Accuracy**: ~99.9%
- **Use Case**: Metro station vicinity demand prediction

### 3. Training Process

```python
# Common training pipeline for all models
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split

# Split data (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Classifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Save model
joblib.dump(model, "model_name.pkl")
```

### 4. Prediction Pipeline

1. User inputs location (lat, lng) and time
2. Time converted to minutes since midnight
3. Features passed to selected model: `[latitude, longitude, time_minutes]`
4. Model outputs binary prediction (0 or 1)
5. Result converted to "High" or "Low" demand

## Project Structure

```
.
├── PeakHourDemand/
│   ├── app.py                          # Flask API server
│   ├── models/                         # Pre-trained ML models
│   │   ├── demand_prediction_model1.pkl
│   │   ├── demand_prediction_model2.pkl
│   │   ├── demand_prediction.pkl
│   │   └── demand_prediction_model.pkl
│   ├── Dataset/                        # Training datasets
│   │   ├── bengaluru-metro-stations.csv
│   │   ├── Cleaned_bangalore_parking.csv
│   │   ├── modified_apartment_data.csv
│   │   ├── company_data.csv
│   │   └── Demand_Training_Data.csv
│   ├── static/                         # Frontend assets
│   │   ├── index.html                  # Main interface
│   │   ├── map.js                      # Map functionality
│   │   ├── styles.css
│   │   └── bangalore_locations.json
│   ├── templates/                      # Additional HTML pages
│   │   ├── all_markers.html
│   │   └── heatmap.html
│   ├── test1.ipynb                     # Model 1 training notebook
│   ├── test2.ipynb                     # Model 4 training notebook
│   ├── test3.ipynb                     # Model 2 training notebook
│   └── test4.ipynb                     # Model 3 training notebook
├── Dataset/                            # Root-level datasets
├── Miscelinous/                        # Documentation and flowcharts
├── Research Papers/                    # Reference papers
└── README.md
```

## Installation

### Prerequisites
- Python 3.8+ (Python 3.10+ recommended)
- pip package manager
- Git

### Setup Instructions

1. **Clone the repository**
```powershell
git clone <repository-url>
cd Cab_demand_prediction-GIH-
```

2. **Create virtual environment**
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
```

3. **Install dependencies**
```powershell
pip install flask joblib numpy pandas scikit-learn
```

Or create a `requirements.txt`:
```txt
flask==3.0.0
joblib==1.4.2
numpy==2.2.1
pandas==2.2.3
scikit-learn==1.6.0
```

Then install:
```powershell
pip install -r requirements.txt
```

## Usage

### Starting the Server

1. Navigate to the project root directory
2. Run the Flask application:
```powershell
python .\PeakHourDemand\app.py
```

3. Access the application:
   - Main interface: `http://127.0.0.1:5000/`
   - All markers view: `http://127.0.0.1:5000/all_markers.html`
   - Heatmap view: `http://127.0.0.1:5000/heatmap.html`

### Web Interface Features

- **Manual Prediction**: Enter latitude, longitude, and time to get demand prediction
- **Model Selection**: Choose from 4 different trained models
- **Interactive Map**: Visualize demand zones on Google Maps
- **Heatmap View**: See demand intensity across the city
- **All Markers View**: Display all prediction points simultaneously

## API Documentation

### Predict Endpoint

**URL**: `/predict`  
**Method**: `POST`  
**Content-Type**: `application/json`

**Request Body**:
```json
{
  "model": "model1",
  "latitude": 12.9716,
  "longitude": 77.5946,
  "time": "09:30"
}
```

**Parameters**:
- `model` (optional): Model selection - "model1", "model2", "model3", or "model4" (default: "model1")
- `latitude` (required): Latitude coordinate (float)
- `longitude` (required): Longitude coordinate (float)
- `time` (required): Time in 24-hour format "HH:MM" (string)

**Response**:
```json
{
  "demand": "High"
}
```

**Example using curl**:
```bash
curl -X POST http://127.0.0.1:5000/predict \
  -H "Content-Type: application/json" \
  -d '{"model":"model1","latitude":12.9716,"longitude":77.5946,"time":"09:30"}'
```

**Example using Python**:
```python
import requests

response = requests.post(
    "http://127.0.0.1:5000/predict",
    json={
        "model": "model1",
        "latitude": 12.9716,
        "longitude": 77.5946,
        "time": "09:30"
    }
)
print(response.json())  # {'demand': 'High'}
```

## Models

| Model | File | Data Source | Peak Hours | Accuracy | Best For |
|-------|------|-------------|------------|----------|----------|
| Model 1 | `demand_prediction_model1.pkl` | Apartments | Variable | 94.9% | Residential areas |
| Model 2 | `demand_prediction_model2.pkl` | Parking lots | 8:00-11:00 AM | 44.9% | Transit hubs |
| Model 3 | `demand_prediction.pkl` | Companies | 5:00-7:00 PM | 95.0% | Corporate zones |
| Model 4 | `demand_prediction_model.pkl` | Metro stations | 8:45-10:20 AM, 4:20-7:00 PM | 99.9% | Metro vicinities |

## Datasets

### Input Datasets
- **bengaluru-metro-stations.csv**: 66 metro stations with coordinates
- **Cleaned_bangalore_parking.csv**: 761 parking locations with timestamps
- **modified_apartment_data.csv**: Residential complexes with timing patterns
- **company_data.csv**: Corporate office locations
- **bmtcBusStop.csv**: BMTC bus stop locations
- **namma_yatri.csv**: Ride-sharing data

### Generated Datasets
- **Demand_Training_Data.csv**: Balanced training data with demand labels

## Features

- **Multi-Model Support**: Choose from 4 specialized models based on location type
- **Real-Time Prediction**: Instant demand forecasting via REST API
- **Interactive Visualization**: Google Maps integration with markers and heatmaps
- **Geospatial Analysis**: Location-based demand pattern recognition
- **Temporal Analysis**: Time-of-day demand prediction
- **Scalable Architecture**: Flask-based API for easy integration

## Technical Details

**Algorithm**: Random Forest Classification
- Ensemble learning method
- 100 decision trees (n_estimators=100)
- Handles non-linear relationships between location and time

**Input Features**: 3 dimensions
1. Latitude (continuous)
2. Longitude (continuous)
3. Time in minutes (discrete: 0-1439)

**Output**: Binary classification
- 1 = High Demand
- 0 = Low Demand

**Performance Metrics**:
- Accuracy: 44.9% - 99.9% depending on model
- Precision, Recall, F1-Score available in training notebooks

## Future Enhancements

- Real-time traffic data integration
- Weather condition impact analysis
- Historical demand trend visualization
- Mobile application development
- Multi-city support
- Deep learning models (LSTM for time-series)

## License

This project is part of academic research. Please refer to the institution's guidelines for usage and distribution.

## Contributors

Developed as part of the GIH (Global Innovation Hub) initiative for optimizing urban transportation in Bangalore.

## References

Research papers and documentation available in the `Research Papers/` and `Miscelinous/` directories.
